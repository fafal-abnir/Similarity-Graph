{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import dataframe_image as dfi"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-data-classification/\n",
    "https://stackoverflow.com/questions/55921286/should-i-balance-the-test-set-when-i-have-highly-unbalanced-data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0  Time        V1        V2        V3        V4        V5  \\\n0           0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321   \n1           1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018   \n2           2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198   \n3           3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309   \n4           4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193   \n\n         V6        V7        V8  ...       V25       V26       V27       V28  \\\n0  0.462388  0.239599  0.098698  ...  0.128539 -0.189115  0.133558 -0.021053   \n1 -0.082361 -0.078803  0.085102  ...  0.167170  0.125895 -0.008983  0.014724   \n2  1.800499  0.791461  0.247676  ... -0.327642 -0.139097 -0.055353 -0.059752   \n3  1.247203  0.237609  0.377436  ...  0.647376 -0.221929  0.062723  0.061458   \n4  0.095921  0.592941 -0.270533  ... -0.206010  0.502292  0.219422  0.215153   \n\n   Class  inverted_dist  fraud_neighbor_count  community_risk  \\\n0      0            0.0                     0        0.002778   \n1      0            0.0                     0        0.000000   \n2      0            0.0                     0        0.000000   \n3      0            0.0                     0        0.001603   \n4      0            0.0                     0        0.002778   \n\n   personalized_page_rank  normAmount  \n0                0.000177    0.244964  \n1                0.000359   -0.342475  \n2                0.000047    1.160686  \n3                0.000064    0.140534  \n4                0.000136   -0.073403  \n\n[5 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>...</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Class</th>\n      <th>inverted_dist</th>\n      <th>fraud_neighbor_count</th>\n      <th>community_risk</th>\n      <th>personalized_page_rank</th>\n      <th>normAmount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.0</td>\n      <td>-1.359807</td>\n      <td>-0.072781</td>\n      <td>2.536347</td>\n      <td>1.378155</td>\n      <td>-0.338321</td>\n      <td>0.462388</td>\n      <td>0.239599</td>\n      <td>0.098698</td>\n      <td>...</td>\n      <td>0.128539</td>\n      <td>-0.189115</td>\n      <td>0.133558</td>\n      <td>-0.021053</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.002778</td>\n      <td>0.000177</td>\n      <td>0.244964</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.191857</td>\n      <td>0.266151</td>\n      <td>0.166480</td>\n      <td>0.448154</td>\n      <td>0.060018</td>\n      <td>-0.082361</td>\n      <td>-0.078803</td>\n      <td>0.085102</td>\n      <td>...</td>\n      <td>0.167170</td>\n      <td>0.125895</td>\n      <td>-0.008983</td>\n      <td>0.014724</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000359</td>\n      <td>-0.342475</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.0</td>\n      <td>-1.358354</td>\n      <td>-1.340163</td>\n      <td>1.773209</td>\n      <td>0.379780</td>\n      <td>-0.503198</td>\n      <td>1.800499</td>\n      <td>0.791461</td>\n      <td>0.247676</td>\n      <td>...</td>\n      <td>-0.327642</td>\n      <td>-0.139097</td>\n      <td>-0.055353</td>\n      <td>-0.059752</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000047</td>\n      <td>1.160686</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1.0</td>\n      <td>-0.966272</td>\n      <td>-0.185226</td>\n      <td>1.792993</td>\n      <td>-0.863291</td>\n      <td>-0.010309</td>\n      <td>1.247203</td>\n      <td>0.237609</td>\n      <td>0.377436</td>\n      <td>...</td>\n      <td>0.647376</td>\n      <td>-0.221929</td>\n      <td>0.062723</td>\n      <td>0.061458</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.001603</td>\n      <td>0.000064</td>\n      <td>0.140534</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2.0</td>\n      <td>-1.158233</td>\n      <td>0.877737</td>\n      <td>1.548718</td>\n      <td>0.403034</td>\n      <td>-0.407193</td>\n      <td>0.095921</td>\n      <td>0.592941</td>\n      <td>-0.270533</td>\n      <td>...</td>\n      <td>-0.206010</td>\n      <td>0.502292</td>\n      <td>0.219422</td>\n      <td>0.215153</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.002778</td>\n      <td>0.000136</td>\n      <td>-0.073403</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/final/creditcard_extra_graph_100_L2_2022-11-26_19:10:54.csv\")\n",
    "df['normAmount'] = StandardScaler().fit_transform(df['Amount'].values.reshape (-1,1))\n",
    "df = df.drop(['Amount'], axis = 1)\n",
    "\n",
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "result_dir = \"../results\"\n",
    "results_name = \"all_previous_in_training\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "GROUP_TIME_LENGTH = 7200\n",
    "max_time = df[\"Time\"].max()\n",
    "group_df = df.groupby(pd.cut(df[\"Time\"], np.arange(-1, max_time + GROUP_TIME_LENGTH, GROUP_TIME_LENGTH)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Distribution of Target class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0    284315\n1       492\nName: Class, dtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Class\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def predict_with_metric(model,X_test, y_test):\n",
    "    prediction = model.predict(X_test)\n",
    "    conf_matrix = confusion_matrix(y_test,prediction)\n",
    "    tn= conf_matrix[0][0]\n",
    "    fp= conf_matrix[0][1]\n",
    "    fn = conf_matrix[1][0]\n",
    "    tp =conf_matrix[1][1]\n",
    "    return {\"Prediction\":list(prediction.flatten()),\"Accuracy\":accuracy_score(y_test, prediction),\"Precision\":precision_score(y_test, prediction),\"Recall\":recall_score(y_test,prediction),\"F1\":f1_score(y_test,prediction),\"TN\":tn,\"FP\":fp,\"FN\":fn,\"TP\":tp}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 µs, sys: 0 ns, total: 28 µs\n",
      "Wall time: 31 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NUM_ESTIMATOR = 50\n",
    "rf_clf = RandomForestClassifier(n_estimators=NUM_ESTIMATOR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Processing group:(-1.0, 7199.0] , group number:1=====\n",
      "===== Processing group:(7199.0, 14399.0] , group number:2=====\n",
      "===== Processing group:(14399.0, 21599.0] , group number:3=====\n",
      "===== Processing group:(21599.0, 28799.0] , group number:4=====\n",
      "Predicting group 4\n",
      "===== Processing group:(28799.0, 35999.0] , group number:5=====\n",
      "Predicting group 5\n",
      "===== Processing group:(35999.0, 43199.0] , group number:6=====\n",
      "Predicting group 6\n",
      "===== Processing group:(43199.0, 50399.0] , group number:7=====\n",
      "Predicting group 7\n",
      "===== Processing group:(50399.0, 57599.0] , group number:8=====\n",
      "Predicting group 8\n",
      "===== Processing group:(57599.0, 64799.0] , group number:9=====\n",
      "Predicting group 9\n",
      "===== Processing group:(64799.0, 71999.0] , group number:10=====\n",
      "Predicting group 10\n",
      "===== Processing group:(71999.0, 79199.0] , group number:11=====\n",
      "Predicting group 11\n",
      "===== Processing group:(79199.0, 86399.0] , group number:12=====\n",
      "Predicting group 12\n",
      "===== Processing group:(86399.0, 93599.0] , group number:13=====\n",
      "Predicting group 13\n",
      "===== Processing group:(93599.0, 100799.0] , group number:14=====\n",
      "Predicting group 14\n",
      "===== Processing group:(100799.0, 107999.0] , group number:15=====\n",
      "Predicting group 15\n",
      "===== Processing group:(107999.0, 115199.0] , group number:16=====\n",
      "Predicting group 16\n",
      "===== Processing group:(115199.0, 122399.0] , group number:17=====\n",
      "Predicting group 17\n",
      "===== Processing group:(122399.0, 129599.0] , group number:18=====\n",
      "Predicting group 18\n",
      "===== Processing group:(129599.0, 136799.0] , group number:19=====\n",
      "Predicting group 19\n",
      "===== Processing group:(136799.0, 143999.0] , group number:20=====\n",
      "Predicting group 20\n",
      "===== Processing group:(143999.0, 151199.0] , group number:21=====\n",
      "Predicting group 21\n",
      "===== Processing group:(151199.0, 158399.0] , group number:22=====\n",
      "Predicting group 22\n",
      "===== Processing group:(158399.0, 165599.0] , group number:23=====\n",
      "Predicting group 23\n",
      "===== Processing group:(165599.0, 172799.0] , group number:24=====\n",
      "Predicting group 24\n"
     ]
    }
   ],
   "source": [
    "normal_acc_l = []\n",
    "normal_pre_l = []\n",
    "normal_rec_l = []\n",
    "normal_f1_l = []\n",
    "normal_tn_l= []\n",
    "normal_fp_l= []\n",
    "normal_fn_l = []\n",
    "normal_tp_l = []\n",
    "group_time_l = []\n",
    "train_data = pd.DataFrame()\n",
    "for group_time, group in group_df:\n",
    "    # print(\"\")\n",
    "    print(f\"===== Processing group:{group_time} , group number:{int((group_time.left + 1) / GROUP_TIME_LENGTH + 1)}=====\")\n",
    "    group_number = int((group_time.left + 1) / GROUP_TIME_LENGTH + 1)\n",
    "    if group_number > 3:\n",
    "        print(f\"Predicting group {group_number}\")\n",
    "        X_test = group[group.columns.difference([\"Class\",\"Time\"])]\n",
    "        y_train = group[\"Class\"]\n",
    "        model_prediction  = predict_with_metric(rf_clf,X_test,y_train)\n",
    "        group_time_l.append(group_time)\n",
    "        normal_acc_l.append(model_prediction[\"Accuracy\"])\n",
    "        normal_pre_l.append(model_prediction[\"Precision\"])\n",
    "        normal_rec_l.append(model_prediction[\"Recall\"])\n",
    "        normal_f1_l.append(model_prediction[\"F1\"])\n",
    "        normal_tn_l.append(model_prediction[\"TN\"])\n",
    "        normal_fp_l.append(model_prediction[\"FP\"])\n",
    "        normal_fn_l.append(model_prediction[\"FN\"])\n",
    "        normal_tp_l.append(model_prediction[\"TP\"])\n",
    "    train_data = pd.concat([train_data,group])\n",
    "    X_train = train_data[train_data.columns.difference([\"Class\",\"Time\"])]\n",
    "    y_train = train_data[\"Class\"]\n",
    "    rf_clf.fit(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '../results'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [10], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m metric_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(group_time_l,normal_acc_l,normal_pre_l,normal_rec_l,normal_f1_l,normal_tn_l,normal_fp_l,normal_fn_l,normal_tp_l))\n\u001B[1;32m      2\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(metric_data, columns \u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mperiod\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPrecision\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRecall\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF-1\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTN\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFP\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFN\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTP\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 3\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mresult_dir\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/dynamic_clf_normal_extra_feature_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mresults_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m dfi\u001B[38;5;241m.\u001B[39mexport(df\u001B[38;5;241m.\u001B[39mstyle\u001B[38;5;241m.\u001B[39mhide_index(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/dynamic_clf_normal_extra_feature_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m df\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/similarity-graph-WKuwPdS3-py3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/similarity-graph-WKuwPdS3-py3.10/lib/python3.10/site-packages/pandas/core/generic.py:3720\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[1;32m   3709\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[1;32m   3711\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[1;32m   3712\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[1;32m   3713\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3717\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[1;32m   3718\u001B[0m )\n\u001B[0;32m-> 3720\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3721\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3722\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3723\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3725\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3726\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3729\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3730\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3731\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3732\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3733\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3734\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3735\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3736\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3737\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/similarity-graph-WKuwPdS3-py3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/similarity-graph-WKuwPdS3-py3.10/lib/python3.10/site-packages/pandas/io/formats/format.py:1189\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[1;32m   1168\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1170\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[1;32m   1171\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[1;32m   1172\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1187\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[1;32m   1188\u001B[0m )\n\u001B[0;32m-> 1189\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/similarity-graph-WKuwPdS3-py3.10/lib/python3.10/site-packages/pandas/io/formats/csvs.py:241\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[0;32m--> 241\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[1;32m    249\u001B[0m \n\u001B[1;32m    250\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[1;32m    252\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[1;32m    253\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    258\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[1;32m    259\u001B[0m     )\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/similarity-graph-WKuwPdS3-py3.10/lib/python3.10/site-packages/pandas/io/common.py:734\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    732\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[1;32m    733\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[0;32m--> 734\u001B[0m     \u001B[43mcheck_parent_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[1;32m    737\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    738\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "File \u001B[0;32m~/.cache/pypoetry/virtualenvs/similarity-graph-WKuwPdS3-py3.10/lib/python3.10/site-packages/pandas/io/common.py:597\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    595\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n\u001B[1;32m    596\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[0;32m--> 597\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mOSError\u001B[0m: Cannot save file into a non-existent directory: '../results'"
     ]
    }
   ],
   "source": [
    "metric_data = list(zip(group_time_l,normal_acc_l,normal_pre_l,normal_rec_l,normal_f1_l,normal_tn_l,normal_fp_l,normal_fn_l,normal_tp_l))\n",
    "df = pd.DataFrame(metric_data, columns =['period','Accuracy', 'Precision', 'Recall','F-1','TN','FP','FN','TP'])\n",
    "df.to_csv(f\"{result_dir}/dynamic_clf_normal_extra_feature_{results_name}.csv\")\n",
    "dfi.export(df.style.hide_index(), f\"{result_dir}/dynamic_clf_normal_extra_feature_{results_name}.png\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Over sampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "NUM_ESTIMATOR = 50\n",
    "rf_clf = RandomForestClassifier(n_estimators=NUM_ESTIMATOR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ros_acc_l = []\n",
    "ros_pre_l = []\n",
    "ros_rec_l = []\n",
    "ros_f1_l = []\n",
    "ros_tn_l= []\n",
    "ros_fp_l= []\n",
    "ros_fn_l = []\n",
    "ros_tp_l = []\n",
    "group_time_l = []\n",
    "train_data = pd.DataFrame()\n",
    "for group_time, group in group_df:\n",
    "    # print(\"\")\n",
    "    print(f\"===== Processing group:{group_time} , group number:{int((group_time.left + 1) / GROUP_TIME_LENGTH + 1)}=====\")\n",
    "    group_number = int((group_time.left + 1) / GROUP_TIME_LENGTH + 1)\n",
    "    if group_number > 3:\n",
    "        print(f\"Predicting group {group_number}\")\n",
    "        X_test = group[group.columns.difference([\"Class\",\"Time\"])]\n",
    "        y_train = group[\"Class\"]\n",
    "        model_prediction  = predict_with_metric(rf_clf,X_test,y_train)\n",
    "        group_time_l.append(group_time)\n",
    "        ros_acc_l.append(model_prediction[\"Accuracy\"])\n",
    "        ros_pre_l.append(model_prediction[\"Precision\"])\n",
    "        ros_rec_l.append(model_prediction[\"Recall\"])\n",
    "        ros_f1_l.append(model_prediction[\"F1\"])\n",
    "        ros_tn_l.append(model_prediction[\"TN\"])\n",
    "        ros_fp_l.append(model_prediction[\"FP\"])\n",
    "        ros_fn_l.append(model_prediction[\"FN\"])\n",
    "        ros_tp_l.append(model_prediction[\"TP\"])\n",
    "    # Random over sampling\n",
    "    train_data = pd.concat([train_data,group])\n",
    "    X_train = train_data[train_data.columns.difference([\"Class\",\"Time\"])]\n",
    "    y_train = train_data[\"Class\"]\n",
    "    ros = RandomOverSampler(random_state=1234)\n",
    "    X_group_ros, y_group_ros = ros.fit_resample(X_train,y_train)\n",
    "    # Check the number of records after over sampling\n",
    "    print(sorted(Counter(y_group_ros).items()))\n",
    "    # Training model\n",
    "    rf_clf.fit(X_train,y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_data = list(zip(group_time_l,ros_acc_l,ros_pre_l,ros_rec_l,ros_f1_l,ros_tn_l,ros_fp_l,ros_fn_l,ros_tp_l))\n",
    "df = pd.DataFrame(metric_data, columns =['period','Accuracy', 'Precision', 'Recall','F-1','TN','FP','FN','TP'])\n",
    "df.to_csv(f\"{result_dir}/dynamic_clf_ros_extra_feature_{results_name}.csv\")\n",
    "dfi.export(df.style.hide_index(), f\"{result_dir}/dynamic_clf_ros_extra_feature_{results_name}.png\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Over sampling with SMOTHE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "NUM_ESTIMATOR = 50\n",
    "rf_clf = RandomForestClassifier(n_estimators=NUM_ESTIMATOR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smote_acc_l = []\n",
    "smote_pre_l = []\n",
    "smote_rec_l = []\n",
    "smote_f1_l = []\n",
    "smote_tn_l= []\n",
    "smote_fp_l= []\n",
    "smote_fn_l = []\n",
    "smote_tp_l = []\n",
    "group_time_l = []\n",
    "train_data = pd.DataFrame()\n",
    "for group_time, group in group_df:\n",
    "    # print(\"\")\n",
    "    print(f\"===== Processing group:{group_time} , group number:{int((group_time.left + 1) / GROUP_TIME_LENGTH + 1)}=====\")\n",
    "    group_number = int((group_time.left + 1) / GROUP_TIME_LENGTH + 1)\n",
    "    if group_number > 3:\n",
    "        print(f\"Predicting group {group_number}\")\n",
    "        X_test = group[group.columns.difference([\"Class\",\"Time\"])]\n",
    "        y_train = group[\"Class\"]\n",
    "        model_prediction  = predict_with_metric(rf_clf,X_test,y_train)\n",
    "        group_time_l.append(group_time)\n",
    "        smote_acc_l.append(model_prediction[\"Accuracy\"])\n",
    "        smote_pre_l.append(model_prediction[\"Precision\"])\n",
    "        smote_rec_l.append(model_prediction[\"Recall\"])\n",
    "        smote_f1_l.append(model_prediction[\"F1\"])\n",
    "        smote_tn_l.append(model_prediction[\"TN\"])\n",
    "        smote_fp_l.append(model_prediction[\"FP\"])\n",
    "        smote_fn_l.append(model_prediction[\"FN\"])\n",
    "        smote_tp_l.append(model_prediction[\"TP\"])\n",
    "    # Random over sampling\n",
    "    train_data = pd.concat([train_data,group])\n",
    "    X_train = train_data[train_data.columns.difference([\"Class\",\"Time\"])]\n",
    "    y_train = train_data[\"Class\"]\n",
    "    smote = SMOTE(random_state=1234,k_neighbors=3)\n",
    "    X_group_smote, y_group_smote = smote.fit_resample(X_train,y_train)\n",
    "    # Check the number of records after over sampling\n",
    "    print(sorted(Counter(y_group_smote).items()))\n",
    "    # Training model\n",
    "    rf_clf.fit(X_train,y_train)\n",
    "    ## pdate train data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_data = list(zip(group_time_l,smote_acc_l,smote_pre_l,smote_rec_l,smote_f1_l,smote_tn_l,smote_fp_l,smote_fn_l,smote_tp_l))\n",
    "df = pd.DataFrame(metric_data, columns =['period','Accuracy', 'Precision', 'Recall','F-1','TN','FP','FN','TP'])\n",
    "df.to_csv(f\"{result_dir}/dynamic_clf_smote_extra_feature_{results_name}.csv\")\n",
    "dfi.export(df.style.hide_index(), f\"{result_dir}/dynamic_clf_smote_extra_feature_{results_name}.png\")\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smote_acc_l"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
